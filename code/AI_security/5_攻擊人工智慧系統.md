# 攻擊人工智慧系統
```

```
## Adversarial example using FGSM
```
快速梯度簽名方法[Fast Gradient Signed Method (FGSM)]攻擊
          ==>建製Adversarial example(對抗樣本)
                   ==>誘使AI做出錯誤判斷

https://www.tensorflow.org/tutorials/generative/adversarial_fgsm
https://s0www0tensorflow0org.icopy.site/tutorials/generative/adversarial_fgsm
```
```
# Commented out IPython magic to ensure Python compatibility.
from __future__ import absolute_import, division, print_function, unicode_literals

try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf
import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams['figure.figsize'] = (8, 8)
mpl.rcParams['axes.grid'] = False

"""Let's load the pretained MobileNetV2 model and the ImageNet class names."""

pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,
                                                     weights='imagenet')
pretrained_model.trainable = False

# ImageNet labels
decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions

# Helper function to preprocess the image so that it can be inputted in MobileNetV2
def preprocess(image):
  image = tf.cast(image, tf.float32)
  image = image/255
  image = tf.image.resize(image, (224, 224))
  image = image[None, ...]
  return image

# Helper function to extract labels from probability vector
def get_imagenet_label(probs):
  return decode_predictions(probs, top=1)[0][0]

"""## Original image
Let's use a sample image of a [Labrador Retriever](https://commons.wikimedia.org/wiki/File:YellowLabradorLooking_new.jpg) -by Mirko       [CC-BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/) from Wikimedia Common and create adversarial examples from it. The first step is to preprocess it so that it can be fed as an input to the MobileNetV2 model.
"""

image_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')
image_raw = tf.io.read_file(image_path)
image = tf.image.decode_image(image_raw)

image = preprocess(image)
image_probs = pretrained_model.predict(image)

"""Let's have a look at the image."""

plt.figure()
plt.imshow(image[0])
_, image_class, class_confidence = get_imagenet_label(image_probs)
plt.title('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100))
plt.show()

"""## Create the adversarial image

### Implementing fast gradient sign method
The first step is to create perturbations which will be used to 
distort the original image resulting in an adversarial image. 
As mentioned, for this task, the gradients are taken with respect to the image.
"""

loss_object = tf.keras.losses.CategoricalCrossentropy()

def create_adversarial_pattern(input_image, input_label):
  with tf.GradientTape() as tape:
    tape.watch(input_image)
    prediction = pretrained_model(input_image)
    loss = loss_object(input_label, prediction)

  # Get the gradients of the loss w.r.t to the input image.
  gradient = tape.gradient(loss, input_image)
  # Get the sign of the gradients to create the perturbation
  signed_grad = tf.sign(gradient)
  return signed_grad

"""The resulting perturbations can also be visualised."""

perturbations = create_adversarial_pattern(image, image_probs)
plt.imshow(perturbations[0])

"""Let's try this out for different values of epsilon and observe the resultant image. 
You'll notice that as the value of epsilon is increased, it becomes easier to fool the network, 
however, this comes as a trade-off which results in the perturbations becoming more identifiable."""

def display_images(image, description):
  _, label, confidence = get_imagenet_label(pretrained_model.predict(image))
  plt.figure()
  plt.imshow(image[0])
  plt.title('{} \n {} : {:.2f}% Confidence'.format(description,
                                                   label, confidence*100))
  plt.show()

epsilons = [0, 0.01, 0.1, 0.15]
descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')
                for eps in epsilons]

for i, eps in enumerate(epsilons):
  adv_x = image + eps*perturbations
  adv_x = tf.clip_by_value(adv_x, 0, 1)
  display_images(adv_x, descriptions[i])

```
# 關鍵程式說明
```
# Commented out IPython magic to ensure Python compatibility.
from __future__ import absolute_import, division, print_function, unicode_literals

try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf
import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams['figure.figsize'] = (8, 8)
mpl.rcParams['axes.grid'] = False
```
### 使用預訓練模型pretained MobileNetV2 mode
```
# 定義根據imagenet資料級所產生的MobileNetV2 預訓練模型
pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,
                                                     weights='imagenet')
pretrained_model.trainable = False

# ImageNet labels
decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions

# Helper function to preprocess the image so that it can be inputted in MobileNetV2
def preprocess(image):
  image = tf.cast(image, tf.float32)
  image = image/255
  image = tf.image.resize(image, (224, 224))
  image = image[None, ...]
  return image

# Helper function to extract labels from probability vector
def get_imagenet_label(probs):
  return decode_predictions(probs, top=1)[0][0]
```
### keras可用的模型
```
https://keras.io/zh/applications/
https://www.tensorflow.org/api_docs/python/tf/keras/applications

Keras 的應用模組（keras.applications）提供了帶有預訓練權值的深度學習模型，
這些模型可以用來進行預測、特徵提取和微調（fine-tuning）。

當你初始化一個預訓練模型時，會自動下載權重到 ~/.keras/models/ 目錄下。

可用的模型
在 ImageNet 上预训练过的用于图像分类的模型：
Xception
VGG16
VGG19
ResNet, ResNetV2, ResNeXt
InceptionV3
InceptionResNetV2
MobileNet
MobileNetV2
DenseNet
NASNet
```
### 原始圖片
```
image_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')

image_raw = tf.io.read_file(image_path)
image = tf.image.decode_image(image_raw)

image = preprocess(image)
image_probs = pretrained_model.predict(image)

"""Let's have a look at the image."""

plt.figure()
plt.imshow(image[0])
_, image_class, class_confidence = get_imagenet_label(image_probs)
plt.title('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100))
plt.show()
```
#### tf.keras.utils.get_file()
```
tf.keras.utils

從遠端網址下載資料，並保存到檔案中。
下載之前先進行檢查，如果檔案存在，就直接讀取。

https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file

tf.keras.utils.get_file(
    fname,  #檔案命名
    origin, #檔案的url位址
    untar=False,
    md5_hash=None,
    file_hash=None,
    cache_subdir='datasets',
    hash_algorithm='auto',   
          有三種選項(options):'md5', 'sha256', and 'auto'. 
          預設使用 'auto'::會偵測the hash algorithm in use.
    extract=False,
    archive_format='auto',
         有四種選項(options):'auto', 'tar', 'zip', and None. 
         'tar' includes tar, tar.gz, and tar.bz files. 
         The default 'auto' is ['tar', 'zip']. 
         None or an empty list will return no matches found.
    cache_dir=None   #存儲位址
)

範例:
train_path = tf.keras.utils.get_file(
fname=TRAIN_URL.split('/')[-1], #檔案命名
origin=TRAIN_URL, 
cache_dir='.'  #存儲到目前的目錄
)

Returns(回傳)：Path to the downloaded file
```
#### tf.io.decode_image()
```
https://www.tensorflow.org/api_docs/python/tf/io/decode_image

tf.io.decode_image(
    contents,
    channels=None,
    dtype=tf.dtypes.uint8,
    name=None,
    expand_animations=True
)

Detects whether an image is a BMP, GIF, JPEG, or PNG, 
and performs the appropriate operation to convert the input bytes string into a Tensor of type dtype

Returns(回傳)：
對於BMP、JPEG和PNG圖像,形狀為[height, width, num_channels]
對於GIF圖像,形狀為[num_frames, height, width, 3]的類型為uint8的Tensor

https://zhuanlan.zhihu.com/p/31085147

tensorflow對圖像的常規操作介面是如何實現的，
包括圖像編解碼、大小重調、裁剪、翻轉、旋轉、移位、色空間轉換、顏色調整、圖像填充及降噪等等處理，
這些實現極大的豐富了我們對圖像的操作選擇，可用於圖像資料處理而不需要使用opencv。
```
#### tf.io.read_file()
```
https://www.tensorflow.org/api_docs/python/tf/io/read_file
http://tensorfly.cn/tfdoc/api_docs/tf/io/read_file.html

Aliases(別名)：
tf.io.read_file
tf.read_file

tf.io.read_file(filename, name=None)
讀取並輸出輸入檔案名的全部內容。

Args(參數)：
filename：string類型的Tensor。
name：操作的名稱（可選）。

Returns(回傳)：string類型的Tensor。
```
### 建立 adversarial 圖片
```
"""

### Implementing fast gradient sign method
The first step is to create perturbations which will be used to 
distort the original image resulting in an adversarial image. 
As mentioned, for this task, the gradients are taken with respect to the image.
"""

loss_object = tf.keras.losses.CategoricalCrossentropy()

def create_adversarial_pattern(input_image, input_label):
  with tf.GradientTape() as tape:
    tape.watch(input_image)
    prediction = pretrained_model(input_image)
    loss = loss_object(input_label, prediction)

  # Get the gradients of the loss w.r.t to the input image.
  gradient = tape.gradient(loss, input_image)
  # Get the sign of the gradients to create the perturbation
  signed_grad = tf.sign(gradient)
  return signed_grad

"""The resulting perturbations can also be visualised."""

perturbations = create_adversarial_pattern(image, image_probs)
plt.imshow(perturbations[0])

"""Let's try this out for different values of epsilon and observe the resultant image. 
You'll notice that as the value of epsilon is increased, it becomes easier to fool the network, 
however, this comes as a trade-off which results in the perturbations becoming more identifiable."""

def display_images(image, description):
  _, label, confidence = get_imagenet_label(pretrained_model.predict(image))
  plt.figure()
  plt.imshow(image[0])
  plt.title('{} \n {} : {:.2f}% Confidence'.format(description,
                                                   label, confidence*100))
  plt.show()

epsilons = [0, 0.01, 0.1, 0.15]
descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')
                for eps in epsilons]

for i, eps in enumerate(epsilons):
  adv_x = image + eps*perturbations
  adv_x = tf.clip_by_value(adv_x, 0, 1)
  display_images(adv_x, descriptions[i])
```

### 
```
GOOGLE 一下
運用對抗例攻擊深度學習模型（一）
運用對抗例攻擊深度學習模型（二）

Attacking Machine Learning with Adversarial Examples
https://openai.com/blog/adversarial-example-research/
```
```
AI Safety — How Do you Prevent Adversarial Attacks?

```
