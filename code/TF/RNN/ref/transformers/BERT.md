#

```
Transfer Learning
Re-use knowledge from pre-trained model

Transfer Learning in NLP
Word2Vec(2014)
Glove(2014)
ULMFit(2018)
ELMo(2018)

BERT 原理與模型架構
https://www.youtube.com/watch?v=gciHGREJdJ4
```
```
Bert: NLP 中文意圖分類
https://studentcodebank.wordpress.com/2019/08/18/bert-nlp-%e4%b8%ad%e6%96%87%e6%84%8f%e5%9c%96%e5%88%86%e9%a1%9e/

https://github.com/Chunshan-Theta/bert-classify-chinese
```

```
[1]Seq2Seq 模型

[2]自注意力機制 Attention-based Model
https://www.youtube.com/watch?v=jd9DtlR90ak&feature=youtu.be

[3]Transformer
https://www.youtube.com/watch?v=ugWDIIOHtPA

https://arxiv.org/abs/1706.03762
Attention Is All You Need
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
(Submitted on 12 Jun 2017 (v1), last revised 6 Dec 2017 (this version, v5))

Transformer 實際上是一種基於自注意力機制的 Seq2Seq 模型，近年在圖像描述、聊天機器人、語音辨識以及機器翻譯等各大領域大發異彩

https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html


GPT   https://openai.com/blog/language-unsupervised/
GPT2   https://openai.com/blog/better-language-models/
https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
https://github.com/openai/gpt-2


Pre-Training with Whole Word Masking for Chinese BERT
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu
(Submitted on 19 Jun 2019)
https://arxiv.org/abs/1906.08101
中文预训练BERT-wwm（Pre-Trained Chinese BERT with Whole Word Masking）
在自然语言处理领域中，预训练模型（Pre-trained Models）已成为非常重要的基础技术。 为了进一步促进中文信息处理的研究发展，我们发布了基于全词遮罩（Whole Word Masking）技术的中文预训练模型BERT-wwm，以及与此技术密切相关的模型：BERT-wwm-ext，RoBERTa-wwm-ext，RoBERTa-wwm-ext-large。 同时在我们的技术报告中详细对比了当今流行的中文预训练模型：BERT、ERNIE、BERT-wwm。 更多细节请参考我们的技术报告：https://arxiv.org/abs/1906.08101
https://github.com/ymcui/Chinese-BERT-wwm

```
```
文本摘要（Text Summarization）
圖像描述（Image Captioning）
閱讀理解（Reading Comprehension）
語音辨識（Voice Recognition）
語言模型（Language Model）
聊天機器人（Chat Bot）
其他任何可以用 RNN 的潛在應用
```

### 假新聞分類
```
WSDM - Fake News Classification
Identify the fake news.
https://www.kaggle.com/c/fake-news-pair-classification-challenge/leaderboard

李宏毅
第十五週Transformer
第十六週BERT

http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html

```
