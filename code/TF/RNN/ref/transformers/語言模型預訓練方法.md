#

```
自然语言处理中的语言模型预训练方法
https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492317&idx=1&sn=e823a75d9463257ed9ea7b3e4677c1ae&chksm=96ea3d5da19db44be0872ff4e29043aa72c7a624a116196bfeeca092a15f9209d7cf8ce46eb5&scene=21#wechat_redirect

预训练语言模型综述
https://www.zhihu.com/topic/19955854/hot

```
### 語言模型（Language Model）
```
https://www.zhihu.com/question/29456588
語言模型簡單來說就是一串詞序列的概率分佈。

語言模型是假設一門語言所有可能的句子服從一個概率分佈，每個句子出現的概率加起來是1，
那麼語言模型的任務就是預測每個句子在語言中出現的概率，

對於語言中常見的句子，一個好的語言模型應該得到相對高的概率，
對不合語法的句子，計算出的概率則趨近於零。

認為下一個詞的出現僅依賴於他前面的一個或者m個詞

具體來說，語言模型的作用是為一個長度為 m 的文本確定一個概率分佈 P，表示這段文本存在的可能性。

在實踐中，如果文本的長度較長，P(wi | w1, w2, . . . , wi−1) 的估算會非常困難。
因此，研究者們提出使用一個簡化模型：n 元模型（n-gram model）。
在 n 元模型中估算條件概率時，只需要對當前詞的前 n 個詞進行計算。
在 n 元模型中，傳統的方法一般採用頻率計數的比例來估算 n 元條件概率。
當 n 較大時，機會存在資料稀疏問題，導致估算結果不準確。
因此，一般在百萬詞級別的語料中，一般也就用到三元模型。
```

### 統計語言模型 vs 神經網路語言模型-NNLM
```
統計語言模型
   統計語言模型中為了減少參數數量，基於瑪律可夫假設，通常採用n-gram模型

神經網路語言模型-NNLM(since 2003)

    語言模型就是根據上下文去預測下一個詞是什麼，
    這不需要人工標注語料，
    所以語言模型能夠從無限制的大規模單語語料中，學習到豐富的語義知識。

共同點：都是計算語言模型，將句子看作一個詞序列，來計算句子的概率

不同點：計算概率方式不同，
       n-gram基於瑪律可夫假設只考慮前n個詞，
       nnlm要考慮整個句子的上下文訓練模型的方式不同，
       n-gram基於最大似然估計來計算參數，
       nnlm基於RNN的優化方法來訓練模型，並且這個過程中往往會有word embedding作為輸入，這樣對於相似的詞可以有比較好的計算結果，

      ??但n-gram是嚴格基於詞本身的迴圈神經網路可以將任意長度的上下文資訊存儲在隱藏狀態中，而不僅限於n-gram模型中的視窗限制
```

### NNLM
```
相對於統計語言模型，NNLM在相似語義（similar semantic）和語法角色（grammatical roles)上進行了優化。

首先看一個例句：

“The cat is walking in the bedroom”

通過NNLM可以產生類似的句子，如：

“A dog was running in a room”

因為兩句話中的(the, a)、(room, bedroom)、(dog, cat)等詞對擁有相似的語義和相同的語法角色，所以才能構造出相似的句子。


NNLM通過分散式表示（distributed representations）解決了維度災難（curse of dimensionality）的問題。
通過詞向量矩陣C，將詞彙表中的V個單詞映射為V個m維的詞向量（feature vector）。

同樣使用n-gram表示，但是NNLM卻是共用參數矩陣C。
相反，統計語言模型卻需要用詞矩陣表示每一個句子，空間代價太大。

Bengio通過1個3層的神經網路來構建NNLM，
    與普通NN不同的是：輸入層的輸入資料實質上是詞向量矩陣C，並且是全域共用的。

```
