#
```

```
```
[1] 注意力機制在自然語言處理中的應用

http://www.cnblogs.com/robert-dlut/p/5952032.html

[2] Attention is All You Need

https://www.paperweekly.site/papers/224

[3] 深度學習中的注意力機制（2017版）

https://blog.csdn.net/malefactor/article/details/78767781

[4] Recurrent Models of Visual Attention

https://www.paperweekly.site/papers/1788

[5] Neural Machine Translation by Jointly Learning to Align and Translate

https://www.paperweekly.site/papers/434


[1] Romain Paulus, Caiming Xiong, and Richard Socher. 
A deep reinforced model for abstractive summarization. 
arXiv preprint arXiv:1705.04304, 2017.

[2] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 
A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.

[3] Jianpeng Cheng, Li Dong, and Mirella Lapata. 
Long short-term memory-networks for machine reading. 
arXiv preprint arXiv:1601.06733, 2016.

[4] Shen, T.; Zhou, T.; Long, G.; Jiang, J.; Pan, S.; and Zhang, C. Disan: 
Directional self-attention network for rnn/cnn-free language understanding. 
arXiv preprint arXiv:1709.04696, 2017.

[5] Im, Jinbae, and Sungzoon Cho. 
Distance-based Self-Attention Network for Natural Language Inference. 
arXiv preprint arXiv:1712.02047, 2017.

[6] Shaw, Peter, Jakob Uszkoreit, and Ashish Vaswani. 
Self-Attention with Relative Position Representations. 
arXiv preprint arXiv:1803.02155 ,2018.
```

### Attention is All You Need (2017)
```
https://arxiv.org/abs/1706.03762


Attention is All You Need | 每周一起读


```
