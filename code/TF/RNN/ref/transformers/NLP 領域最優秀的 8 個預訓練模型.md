#
```
NLP 領域最優秀的 8 個預訓練模型
https://kknews.cc/code/ey94pkr.html

[1]多用途自然語言處理模型
ULMFiT
Transformer
Google BERT
Transformer-XL
OpenAI GPT-2

[2]詞嵌入
ELMo
Flair

[3]其他預訓練模型
StanfordNLP
```
```
自然語言處理頂會NAACL最佳論文谷歌BERT名至實歸獲最佳長論文
2019-04-12 
原文網址：https://kknews.cc/tech/x2ej5rq.html
```
```
自然語言處理應用能夠快速增長，很大程度上要歸功於通過預訓練模型實現遷移學習的概念。
在自然語言處理的背景下，遷移學習本質上是在一個數據集上訓練模型，然後對該模型進行調整，以在不同的數據集上執行不同的自然語言處理功能。

這一突破，使得每個人都能夠輕鬆地完成任務，尤其是那些沒有時間、也沒有資源從頭開始構建自然語言處理模型的人們。
對於想要學習或過渡到自然語言處理的初學者來講，它也堪稱完美。

為什麼要使用預訓練模型？
作者已盡其所能設計了基準模型。
我們可以在自己的自然語言處理數據集上使用預訓練模型，而不是從頭構建模型來解決類似的自然語言處理問題。
儘管仍然需要進行一些微調，但它已經為我們節省了大量的時間和計算資源。

在本文中，我將介紹一些頂級的預訓練模型，你可以用它們來開始你的自然語言處理之旅，並複製該領域的最新研究成果。

如果你是自然語言處理的初學者，我建議你參加我們的熱門課程：
《NLP using Python》（《使用 Python 進行自然語言處理》）：
https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&utm_medium=top-pretrained-models-nlp-article*


原文網址：https://kknews.cc/code/ey94pkr.html
```
### ULMFiT()

```
Universal Language Model Fine-tuning for Text Classification
Jeremy Howard, Sebastian Ruder
(Submitted on 18 Jan 2018 (v1), last revised 23 May 2018 (this version, v5))
https://arxiv.org/abs/1801.06146

《Tutorial on Text Classification (NLP) using ULMFiT and fastai Library in Python》
《在 Python 中使用 ULMFiT 和 fastai 庫的文本分類（自然語言處理）教程》：
https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/
?utm_source=blog&utm_medium=top-pretrained-models-nlp-article
ULMFiT 的預訓練模型：https://www.paperswithcode.com/paper/universal-language-model-fine-tuning-for-text

https://kknews.cc/code/ey94pkr.html
```
