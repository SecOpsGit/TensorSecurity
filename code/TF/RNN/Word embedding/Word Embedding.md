# Word Embedding
```
詞嵌入向量(Word Embedding)是NLP裡面一個重要的概念，

我們可以利用 WordEmbedding 

將一個單詞轉換成固定長度的向量表示，從而便於進行數學處理
```
```
https://www.mashangxue123.com/tensorflow/tf2-tutorials-text-word_embeddings.html
```

### Word ====> Vector 
```
底下幾種方法:

1. 將文本表示為數位
機器學習模型以向量（數位陣列）作為輸入，
在處理文本時，我們必須首先想出一個策略，將字串轉換為數位（或將文本“向量化”），
然後再將其提供給模型。

在本節中，我們將研究三種策略。

1.1. 獨熱編碼（One-hot encodings）
首先，我們可以用“one-hot”對詞彙的每個單詞進行編碼，
想想“the cat sat on the mat”這句話，
這個句子中的詞彙（或獨特的單詞）是（cat,mat,on,The），
為了表示每個單詞，我們將創建一個長度等於詞彙表的零向量，
然後再對應單詞的索引中放置一個1。

為了創建包含句子編碼的向量，我們可以連接每個單詞的one-hot向量。

關鍵點：這種方法是低效的，一個熱編碼的向量是稀疏的（意思是，大多數指標是零）。
假設我們有10000個單詞，要對每個單詞進行一個熱編碼，我們將創建一個向量，其中99.99%的元素為零。

1.2. 用唯一的數位編碼每個單詞
我們嘗試第二種方法，使用唯一的數位編碼每個單詞。
繼續上面的例子，我們可以將1賦值給“cat”，將2賦值給“mat”，以此類推，
然後我們可以將句子“The cat sat on the mat”編碼為像[5, 1, 4, 3, 5, 2]這樣的密集向量。

這種方法很有效，我們現有有一個稠密的向量（所有元素都是滿的），而不是稀疏的向量。

然而，這種方法有兩個缺點：
[1]整數編碼是任意的（它不捕獲單詞之間的任何關係）。
[2]對於模型來說，整數編碼的解釋是很有挑戰性的。
    例如，線性分類器為每個特徵學習單個權重。
    由於任何兩個單詞的相似性與它們編碼的相似性之間沒有關係，所以這種特徵權重組合沒有意義。

1.3. 詞嵌入word embedding
詞嵌入為我們提供了一種使用高效、密集表示的方法，其中相似的單詞具有相似的編碼，
重要的是，我們不必手工指定這種編碼，嵌入是浮點值的密集向量（向量的長度是您指定的參數），
它們不是手工指定嵌入的值，而是可訓練的參數（模型在訓練期間學習的權重，與模型學習密集層的權重的方法相同）。
通常會看到8維（對於小資料集）的詞嵌入，在處理大型資料集時最多可達1024維。 
更高維度的嵌入可以捕獲單詞之間的細細微性關係，但需要更多的資料來學習。


上面是詞嵌入的圖表，每個單詞表示為浮點值的4維向量，
另一種考慮嵌入的方法是“查閱資料表”，在學習了這些權重之後，
我們可以通過查閱資料表中對應的密集向量來編碼每個單詞。
```
