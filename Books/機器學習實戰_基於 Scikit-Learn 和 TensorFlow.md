#
```
機器學習實戰：基於 Scikit-Learn 和 TensorFlow 
Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for Building Intelligent Systems
机器学习实战:基于Scikit-Learn和TensorFlow
奧雷利安·傑龍 (Aurélien Géron)
機械工業出版社 2018-08-29

https://github.com/ageron/handson-ml
```
```
第一部分 機器學習基礎
第1章 機器學習概覽11
什麼是機器學習12
為什麼要使用機器學習12
機器學習系統的種類15
監督式/無監督式學習16
批量學習和線上學習21
基於實例與基於模型的學習24
機器學習的主要挑戰29
訓練資料的數量不足29
訓練資料不具代表性30
品質差的資料32
無關特徵32
訓練數據過度擬合33
訓練數據擬合不足34
退後一步35
測試與驗證35
練習37


第2章 端到端的機器學習項目39
使用真實資料39
觀察大局40
框架問題41
選擇性能指標42
檢查假設45
獲取資料45
創建工作區45
下載數據48
快速查看資料結構49
創建測試集52
從資料探索和視覺化中獲得洞見56
將地理資料視覺化57
尋找相關性59
試驗不同屬性的組合61
機器學習演算法的資料準備62
資料清理63
處理文本和分類屬性65
自訂轉換器67
特徵縮放68
轉換流水線68
選擇和訓練模型70
培訓和評估訓練集70
使用交叉驗證來更好地進行評估72
微調模型74
網格搜索74
隨機搜索76
集成方法76
分析最佳模型及其錯誤76
通過測試集評估系統77
啟動、監控和維護系統78
試試看79
練習79


第3章 分類80
MNIST80
訓練一個二元分類器82
性能考核83
使用交叉驗證測量精度83
混淆矩陣84
精度和召回率86
精度/召回率權衡87
ROC曲線90
多類別分類器93
錯誤分析95
多標籤分類98
多輸出分類99
練習100


第4章 訓練模型102
線性回歸103
標準方程104
計算複雜度106
梯度下降107
批量梯度下降110
隨機梯度下降112
小批量梯度下降114
多項式回歸115
學習曲線117
正則線性模型121
嶺回歸121
套索回歸123
彈性網路125
早期停止法126
邏輯回歸127
概率估算127
訓練和成本函數128
決策邊界129
Softmax回歸131
練習134


第5章 支持向量機136
線性SVM分類136
軟間隔分類137
非線性SVM分類139
多項式核140
添加相似特徵141
高斯RBF核函數142
計算複雜度143
SVM回歸144
工作原理145
決策函數和預測146
訓練目標146
二次規劃148
對偶問題149
核化SVM149
線上SVM151
練習152


第6章 決策樹154
決策樹訓練和視覺化154
做出預測155
估算類別概率157
CART訓練演算法158
計算複雜度158
基尼不純度還是資訊熵159
正則化超參數159
回歸161
不穩定性162
練習163


第7章 集成學習和隨機森林165
投票分類器165
bagging和pasting168
Scikit-Learn的bagging和pasting169
包外評估170
Random Patches和隨機子空間171
隨機森林172
極端隨機樹173
特徵重要性173
提升法174
AdaBoost175
梯度提升177
堆疊法181
練習184


第8章 降維185
維度的詛咒186
資料降維的主要方法187
投影187
流形學習189
PCA190
保留差異性190
主成分191
低維度投影192
使用Scikit-Learn192
方差解釋率193
選擇正確數量的維度193
PCA壓縮194
增量PCA195
隨機PCA195
核主成分分析196
選擇核函數和調整超參數197
局部線性嵌入199
其他降維技巧200
練習201


第二部分 神經網路和深度學習
第9章 運行TensorFlow205
安裝207
創建一個計算圖並在會話中執行208
管理圖209
節點值的生命週期210
TensorFlow中的線性回歸211
實現梯度下降211
手工計算梯度212
使用自動微分212
使用優化器214
給訓練演算法提供資料214
保存和恢復模型215
用TensorBoard來視覺化圖和訓練曲線216
命名作用域219
模組化220
共用變數222


第10章 人工神經網路簡介227
從生物神經元到人工神經元227
生物神經元228
具有神經元的邏輯計算229
感知器230
多層感知器和反向傳播233
用TensorFlow的高級API來訓練MLP236
使用純TensorFlow訓練DNN237
構建階段237
執行階段240
使用神經網路241
微調神經網路的超參數242
隱藏層的個數242
每個隱藏層中的神經元數243
啟動函數243


第11章 訓練深度神經網路245
梯度消失/爆炸問題245
Xavier初始化和He初始化246
非飽和啟動函數248
批量歸一化250
梯度剪裁254
重用預訓練圖層255
重用TensorFlow模型255
重用其他框架的模型256
凍結低層257
緩存凍結層257
調整、丟棄或替換高層258
模型動物園258
無監督的預訓練259
輔助任務中的預訓練260
快速優化器261
Momentum優化261
Nesterov梯度加速262
AdaGrad263
RMSProp265
Adam優化265
學習速率調度267
通過正則化避免過度擬合269
提前停止269
1和2正則化269
dropout270
最大範數正則化273
資料擴充274
實用指南275
練習276


第12章 跨設備和伺服器的分散式TensorFlow279
一台機器上的多個運算資源280
安裝280
管理GPU RAM282
在設備上操作284
並存執行287
控制依賴288
多設備跨多伺服器288
開啟一個會話290
master和worker服務290
分配跨任務操作291
跨多參數伺服器分片變數291
用資源容器跨會話共用狀態292
使用TensorFlow佇列進行非同步通信294
直接從圖中載入資料299
在TensorFlow集群上並行化神經網路305
一台設備一個神經網路305
圖內與圖間複製306
模型並行化308
資料並行化309


第13章 卷積神經網路31

```
