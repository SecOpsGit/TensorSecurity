#
```
從機器學習到深度學習：基於scikit-learn與TensorFlow的高效開發實戰)
劉長龍  電子工業2019.03
` ``

```
第1章 機器學習基礎 1 

1.1 引言 1 

1.1.1 為什麼使用機器學習 2 

1.1.2 機器學習與資料採擷 4 

1.1.3 機器學習與人工智慧 5 

1.2 機器學習的一般流程 7 

1.2.1 定義問題 7 

1.2.2 收集資料 8 

1.2.3 比較演算法與模型 9 

1.2.4 應用模型 10 

1.3 學習策略 10 

1.3.1 有監督學習 11 

1.3.2 無監督學習 14 

1.3.3 強化學習 16 

1.3.4 綜合模型與工具 18 

1.4 評估理論 19 

1.4.1 劃分資料集 19 

1.4.2 交叉驗證 21 

1.4.3 評估指標 22 

1.4.4 擬合不足與過度擬合 25 

1.5 本章內容回顧 26 

第2章 Python基礎工具 27 

2.1 Numpy 28 

2.1.1 Numpy與Scipy的分工 28 

2.1.2 ndarray構造 29 

2.1.3 資料類型 32 

2.1.4 訪問與修改 33 

2.1.5 軸 35 

2.1.6 維度操作 38 

2.1.7 合併與拆分 40 

2.1.8 增與刪 41 

2.1.9 全函數 42 

2.1.10 廣播 42 

2.2 Matplot 43 

2.2.1 點線圖 44 

2.2.2 子視圖 50 

2.2.3 圖像 53 

2.2.4 等值圖 57 

2.2.5 三維繪圖 58 

2.2.6 從官網學習 59 

2.3 Scipy 60 

2.3.1 數學與物理常數 61 

2.3.2 特殊函式程式庫 62 

2.3.3 積分 64 

2.3.4 優化 65 

2.3.5 插值 67 

2.3.6 離散傅裡葉 68 

2.3.7 卷積 70 

2.3.8 線性分析 71 

2.3.9 概率統計 73 

2.4 本章內容回顧 77 

第3章 有監督學習：分類與回歸 79 

3.1 線性回歸 80 

3.1.1 何謂線性模型 80 

3.1.2 最小二乘法 81 

3.1.3 最小二乘法的不足 82 

3.1.4 嶺回歸 85 

3.1.5 Lasso回歸 87 

3.2 梯度下降 90 

3.2.1 假設函數與損失函數 90 

3.2.2 隨機梯度下降 92 

3.2.3 實戰：SGDRegressor和SGDClassifier 93 

3.2.4 增量學習 94 

3.3 支持向量機 95 

3.3.1 最優超平面 95 

3.3.2 軟間隔 97 

3.3.3 線性不可分問題 98 

3.3.4 核函數 99 

3.3.5 實戰：scikit-learn中的SVM 100 

3.4 樸素貝葉斯分類 101 

3.4.1 基礎概率 102 

3.4.2 貝葉斯分類原理 103 

3.4.3 高斯樸素貝葉斯 105 

3.4.4 多項式樸素貝葉斯 106 

3.4.5 伯努利樸素貝葉斯 107 

3.5 高斯過程 107 

3.5.1 隨機過程 108 

3.5.2 無限維高斯分佈 109 

3.5.3 實戰：gaussian_process工具包 111 

3.6 決策樹 114 

3.6.1 最易於理解的模型 114 

3.6.2 熵的作用 115 

3.6.3 實戰：DecisionTreeClassifier與DecisionTreeRegressor 117 

3.6.4 樹的視覺化 118 

3.7 集成學習 119 

3.7.1 偏差與方差 120 

3.7.2 隨機森林 121 

3.7.3 自我調整增強 124 

3.8 綜合話題 126 

3.8.1 參數與非參數學習 127 

3.8.2 One-Vs-All與One-Vs-One 127 

3.8.3 評估工具 129 

3.8.4 超參數調試 131 

3.8.5 多路輸出 134 

3.9 本章內容回顧 134 

第4章 無監督學習：聚類 136 

4.1 動機 137 

4.2 K-means 138 

4.2.1 演算法 139 

4.2.2 實戰：scikit-learn聚類調用 141 

4.2.3 如何選擇K值 144 

4.3 近鄰演算法 145 

4.3.1 生活化的理解 145 

4.3.2 有趣的反覆運算 146 

4.3.3 實戰：AffinityPropagation類 147 

4.4 高斯混合模型 149 

4.4.1 中心極限定理 150 

4.4.2 最大似然估計 151 

4.4.3 幾種協方差矩陣類型 152 

4.4.4 實戰：GaussianMixture類 154 

4.5 密度聚類 156 

4.5.1 凸資料集 157 

4.5.2 密度演算法 158 

4.5.3 實戰：DBSCAN類 159 

4.6 BIRCH 160 

4.6.1 層次模型綜述 161 

4.6.2 聚類特徵樹 162 

4.6.3 實戰：BIRCH相關調用 164 

4.7 距離計算 166 

4.7.1 閔氏距離 166 

4.7.2 馬氏距離 167 

4.7.3 余弦相似度 168 

4.7.4 時間序列比較 169 

4.7.5 傑卡德相似度 169 

4.8 聚類評估 170 

4.9 本章內容回顧 172 

第5章 無監督學習：資料降維 173 

5.1 主成分分析 174 

5.1.1 尋找方差最大維度 174 

5.1.2 用PCA降維 177 

5.1.3 實戰：用PCA尋找主成分 178 

5.2 線性判別分析 181 

5.2.1 雙重標準 181 

5.2.2 實戰：使用LinearDiscriminantAnalysis 183 

5.3 多維標度法 185 

5.3.1 保留距離資訊的線性變換 185 

5.3.2 MDS的重要變形 187 

5.3.3 實戰：使用MDS類 188 

5.4 流形學習之Isomap 189 

5.4.1 什麼是流形 190 

5.4.2 測地線距離 192 

5.4.3 實戰：使用Isomap類 193 

5.5 流形學習之局部嵌入 195 

5.5.1 局部線性嵌入 195 

5.5.2 拉普拉斯特徵映射（LE） 198 

5.5.3 調用介紹 200 

5.5.4 譜聚類 201 

5.6 流形學習之t-SNE 203 

5.6.1 用Kullback-Leiber衡量分佈相似度 203 

5.6.2 為什麼是t-分佈 205 

5.6.3 實戰：使用TSNE類 206 

5.7 實戰：降維模型之比較 207 

5.8 本章內容回顧 210 

第6章 隱瑪律可夫模型 212 

6.1 場景建模 213 

6.1.1 兩種狀態鏈 213 

6.1.2 兩種概率 215 

6.1.3 三種問題 217 

6.1.4 hmmLearn介紹 218 

6.2 離散型分佈演算法與應用 222 

6.2.1 前向演算法與後向演算法 222 

6.2.2 MultinomialNB求估計問題 226 

6.2.3 Viterbi演算法 227 

6.2.4 MultinomialNB求解碼問題 229 

6.2.5 EM演算法 232 

6.2.6 Baum-Welch演算法 233 

6.2.7 用hmmLearn訓練資料 235 

6.3 連續型概率分佈 236 

6.3.1 多元高斯分佈 237 

6.3.2 GaussianHMM 239 

6.3.3 GMMHMM 240 

6.4 實戰：股票預測模型 241 

6.4.1 資料模型 241 

6.4.2 目標 243 

6.4.3 訓練模型 243 

6.4.4 分析模型參數 245 

6.4.5 視覺化短線預測 247 

6.5 本章內容回顧 250 

第7章 貝葉斯網路 251 

7.1 什麼是貝葉斯網路 252 

7.1.1 典型貝葉斯問題 252 
7.1.2 靜態結構 253 
7.1.3 聯合／邊緣／條件概率換算 256 
7.1.4 鏈式法則與變數消元 258 

7.2 網路構建 259 
7.2.1 網路參數估計 260 
7.2.2 啟發式搜索 261 
7.2.3 Chow-Liu Tree演算法 262 

7.3 近似推理 263 
7.3.1 蒙特卡洛方法 264 
7.3.2 瑪律可夫鏈收斂定理 265 
7.3.3 MCMC推理框架 267 
7.3.4 Gibbs採樣 268 
7.3.5 變分貝葉斯 268 

7.4 利用共軛建模 270 
7.4.1 共軛分佈 270 
7.4.2 隱含變數與顯式變數 272 

7.5 實戰：胸科疾病診斷 274 
7.5.1 診斷需求 274 
7.5.2 Python概率工具包 275 
7.5.3 建立模型 276 
7.5.4 MCMC採樣分析 278 
7.5.5 近似推理 281 

第8章 自然語言處理 284 

8.1 文本建模 285 
8.1.1 聊天機器人原理 285 
8.1.2 詞袋模型 286 
8.1.3 訪問新聞資源庫 287 
8.1.4 TF-IDF 290 
8.1.5 實戰：關鍵字推舉 290 

8.2 詞彙處理 294 
8.2.1 中文分詞 294
8.2.2 Word2vec 296 
8.2.3 實戰：尋找近似詞 298 

8.3 主題模型 303 
8.3.1 三層模型 303 
8.3.2 非負矩陣分解 304 
8.3.3 潛在語意分析 305 
8.3.4 隱含狄利克雷分配 307 
8.3.5 實戰：使用工具包 309 

8.4 實戰：用LDA分析新聞庫 311 
8.4.1 文本預處理 311 
8.4.2 訓練與顯示 313 
8.4.3 困惑度調參 315 

第9章 深度學習 319 
9.1 神經網路基礎 320 
9.1.1 人工神經網路 320 
9.1.2 神經元與啟動函數 321 
9.1.3 反向傳播 323 
9.1.4 萬能網路 325 

9.2 TensorFlow核心應用 328 
9.2.1 張量 329 
9.2.2 開發架構 331 
9.2.3 資料管理 332 
9.2.4 評估器 335 
9.2.5 圖與會話 338 
9.2.6 逐代（epoch）訓練 341 
9.2.7 圖與統計視覺化 343 

9.3 卷積神經網路 349 
9.3.1 給深度學習一個理由 349 
9.3.2 CNN結構發展 351 
9.3.3 卷積層 354 
9.3.4 池化層 356 
9.3.5 ReLU與Softmax 357 
9.3.6 Inception與ResNet 359 

9.4 優化 362 
9.4.1 批次規範化 362 
9.4.2 剪枝 364 
9.4.3 演算法選擇 366 

9.5 迴圈神經網路與遞迴神經網路 367 
9.5.1 迴圈神經網路 368 
9.5.2 長短期記憶（LSTM） 371 
9.5.3 遞迴神經網路 374 

9.6 前沿精選 377 
9.7 CNN實戰：圖像識別 385 
9.8 RNN實戰：寫詩機器人 397  

第10章 強化學習 418 
10.1 場景與原理 419 
10.2 OpenAI Gym 427 
10.3 深度強化學習 435 
10.4 博弈原理 444 
10.5 實戰：中國象棋版AlphaGo Zero 449 


第11章 模型遷移 478 
11.1 走向移動端 478 
11.2 遷移學習 483 
11.3 案例實戰：基於TensorFlow Hub的遷移學習開發 485 
11.4 本章內容回顧 488 

```
