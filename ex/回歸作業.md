# DataSet:
```
https://raw.githubusercontent.com/MyDearGreatTeacher/AI201909/master/data/housing.csv
```
# 測試演算法

```

```
# sklearn.linear_model

# LinearRegression
```
OLS(Ordinary Least Squares)最小平方和線性回歸的最佳化目標函式就是尋找一個平面，
使得預測與實際值的誤差平方和(Sum of Squared Error, SSE)最小化
```
```
from pandas import read_csv
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression


filename = 'housing.csv'
names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',
         'RAD', 'TAX', 'PRTATIO', 'B', 'LSTAT', 'MEDV']
data = read_csv(filename, names=names, delim_whitespace=True)


array = data.values
X = array[:, 0:13]
Y = array[:, 13]
n_splits = 10
seed = 7
kfold = KFold(n_splits=n_splits, random_state=seed)
model = LinearRegression()

scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('Linear Regression: %.3f' % result.mean())
```

# Regularized Regression 正規化回歸
```

使用 **正規化回歸|regularized regression|penalized models|shrinkage method**來對回歸係數做管控。
正規化回歸模型會對回歸係數大小做出約束，並逐漸的將回歸係數壓縮到零。
而對回歸係數的限制將有助於降低係數的幅度和波動，並降低模型的變異。

正規化回歸的目標函式與OLS回歸類似，但多了一個懲罰參數(penalty parameter, P)：L1 或L2

minimize{SSE+P}

常見的懲罰係數有兩種:
ridge回歸模型: L2 Penalty懲罰參數(平方)
lasso回歸模型: L1 Penalty懲罰參數(絕對值:一次方)
   Least absolute shrinkage and selection operator( Tibshirani, 1996)

效果是類似的。

懲罰係數將會限制回歸係數的大小，除非該變數可以使誤差平方和(SSE)降低對應水準，該特徵係數才會上升
```
### Lasso
```
from sklearn.linear_model import Lasso
model = Lasso()
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('Lasso Regression: %.3f' % result.mean())
```

### Ridge
```
from sklearn.linear_model import Ridge
model = Ridge()
scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('Ridge Regression: %.3f' % result.mean())
```

### ElasticNet:L2 Penalty+L1 Penalty
```
from sklearn.linear_model import ElasticNet
model = ElasticNet()
scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('ElasticNet Regression: %.3f' % result.mean())
```

# 其他

### KNeighbors回歸
```
from sklearn.neighbors import KNeighborsRegressor

scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('KNeighbors Regression: %.3f' % result.mean())
```

### DecisionTree決策樹回歸
```
from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor()
scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('CART: %.3f' % result.mean())
```


###  SVM回歸:SVR
```
from sklearn.svm import SVR

model = SVR()
scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('SVM: %.3f' % result.mean())
```

